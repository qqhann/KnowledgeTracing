{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 1.0.1.post2\n",
      "Using Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pack_sequence, pad_packed_sequence, pad_sequence\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "from math import log, ceil\n",
    "from typing import List, Tuple, Set, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.data import prepare_data, prepare_seq2seq_data, SOURCE_ASSIST0910_SELF, SOURCE_ASSIST0910_ORIG\n",
    "from src.utils import sAsMinutes, timeSince\n",
    "\n",
    "sns.set()\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette('Set1')\n",
    "\n",
    "# =========================\n",
    "# PyTorch version & GPU setup\n",
    "# =========================\n",
    "print('PyTorch:', torch.__version__)\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# dev = torch.device('cpu')\n",
    "print('Using Device:', dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/qqhann/qqhann-paper/ECML2019/dkt_neo/notebook')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirname = Path().resolve()\n",
    "dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1519, 20])\n",
      "torch.Size([386, 20])\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Seed\n",
    "# =========================\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# =========================\n",
    "# Parameters\n",
    "# =========================\n",
    "# model_name = 'RNN'\n",
    "sequence_size = 20\n",
    "epoch_size = 500\n",
    "lr = 0.1\n",
    "batch_size, n_hidden, n_skills, n_layers = 100, 200, 124, 1\n",
    "n_output = n_skills\n",
    "PRESERVED_TOKENS = 2  # PAD, SOS\n",
    "onehot_size = n_skills * 2 + PRESERVED_TOKENS\n",
    "n_input = ceil(log(2 * n_skills))\n",
    "# n_input = onehot_size  #\n",
    "\n",
    "NUM_EMBEDDIGNS, ENC_EMB_DIM, ENC_DROPOUT = onehot_size, n_input, 0.6\n",
    "OUTPUT_DIM, DEC_EMB_DIM, DEC_DROPOUT = onehot_size, n_input, 0.6\n",
    "# OUTPUT_DIM = n_output = 124  # TODO: ほんとはこれやりたい\n",
    "HID_DIM, N_LAYERS = n_hidden, n_layers\n",
    "\n",
    "# =========================\n",
    "# Data\n",
    "# =========================\n",
    "train_dl, eval_dl = prepare_seq2seq_data(\n",
    "    SOURCE_ASSIST0910_ORIG, n_skills, PRESERVED_TOKENS, min_n=3, max_n=sequence_size, batch_size=batch_size, device=dev, sliding_window=1)\n",
    "\n",
    "# 違いを調整する <- ???\n",
    "#OUTPUT_DIM = eval_dl.dataset.tensors[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 386,050 trainable parameters\n",
      "Using Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:dkt log:TRAIN Epoch: 110 Loss: 0.6026374141375224\n",
      "INFO:dkt log:EVAL  Epoch: 110 Loss: 0.5924338102340698\n",
      "INFO:dkt log:EVAL  Epoch: 110 AUC: 0.7279595830867963\n",
      "INFO:dkt log:4m 13s ( - 14m 57s) (110 22.0)\n",
      "INFO:dkt log:TRAIN Epoch: 120 Loss: 0.5955672979354858\n",
      "INFO:dkt log:EVAL  Epoch: 120 Loss: 0.5889008045196533\n",
      "INFO:dkt log:EVAL  Epoch: 120 AUC: 0.7289609030085111\n",
      "INFO:dkt log:8m 5s ( - 25m 37s) (120 24.0)\n",
      "INFO:dkt log:TRAIN Epoch: 130 Loss: 0.5934534351030986\n",
      "INFO:dkt log:EVAL  Epoch: 130 Loss: 0.586461583773295\n",
      "INFO:dkt log:EVAL  Epoch: 130 AUC: 0.7285967866733422\n",
      "INFO:dkt log:12m 10s ( - 34m 40s) (130 26.0)\n",
      "INFO:dkt log:TRAIN Epoch: 140 Loss: 0.5886378367741902\n",
      "INFO:dkt log:EVAL  Epoch: 140 Loss: 0.5849830309549967\n",
      "INFO:dkt log:EVAL  Epoch: 140 AUC: 0.7282781848800692\n",
      "INFO:dkt log:16m 12s ( - 41m 39s) (140 28.000000000000004)\n",
      "INFO:dkt log:TRAIN Epoch: 150 Loss: 0.588614288965861\n",
      "INFO:dkt log:EVAL  Epoch: 150 Loss: 0.5844002962112427\n",
      "INFO:dkt log:EVAL  Epoch: 150 AUC: 0.7285967866733422\n",
      "INFO:dkt log:20m 30s ( - 47m 52s) (150 30.0)\n",
      "INFO:dkt log:TRAIN Epoch: 160 Loss: 0.5857635140419006\n",
      "INFO:dkt log:EVAL  Epoch: 160 Loss: 0.5828048785527548\n",
      "INFO:dkt log:EVAL  Epoch: 160 AUC: 0.7298256793045378\n",
      "INFO:dkt log:24m 52s ( - 52m 50s) (160 32.0)\n",
      "INFO:dkt log:TRAIN Epoch: 170 Loss: 0.5825164834658305\n",
      "INFO:dkt log:EVAL  Epoch: 170 Loss: 0.5812955498695374\n",
      "INFO:dkt log:EVAL  Epoch: 170 AUC: 0.7321014063993446\n",
      "INFO:dkt log:28m 46s ( - 55m 50s) (170 34.0)\n",
      "INFO:dkt log:TRAIN Epoch: 180 Loss: 0.5819094101587932\n",
      "INFO:dkt log:EVAL  Epoch: 180 Loss: 0.5804280440012614\n",
      "INFO:dkt log:EVAL  Epoch: 180 AUC: 0.7345136771198397\n",
      "INFO:dkt log:32m 39s ( - 58m 4s) (180 36.0)\n",
      "INFO:dkt log:TRAIN Epoch: 190 Loss: 0.5782005906105041\n",
      "INFO:dkt log:EVAL  Epoch: 190 Loss: 0.5792542099952698\n",
      "INFO:dkt log:EVAL  Epoch: 190 AUC: 0.7360611715443084\n",
      "INFO:dkt log:36m 34s ( - 59m 40s) (190 38.0)\n",
      "Level 25:dkt log:TRAIN Epoch: 200 Loss: 0.5750689109166464\n",
      "Level 25:dkt log:EVAL  Epoch: 200 Loss: 0.5780489246050516\n",
      "Level 25:dkt log:EVAL  Epoch: 200 AUC: 0.7391561603932457\n",
      "INFO:dkt log:40m 29s ( - 60m 44s) (200 40.0)\n",
      "INFO:dkt log:TRAIN Epoch: 210 Loss: 0.5757117629051208\n",
      "INFO:dkt log:EVAL  Epoch: 210 Loss: 0.5779151717821757\n",
      "INFO:dkt log:EVAL  Epoch: 210 AUC: 0.7404760821082337\n",
      "INFO:dkt log:44m 27s ( - 61m 24s) (210 42.0)\n",
      "INFO:dkt log:TRAIN Epoch: 220 Loss: 0.572522513071696\n",
      "INFO:dkt log:EVAL  Epoch: 220 Loss: 0.5771822532018026\n",
      "INFO:dkt log:EVAL  Epoch: 220 AUC: 0.7411132856947795\n",
      "INFO:dkt log:48m 26s ( - 61m 39s) (220 44.0)\n",
      "INFO:dkt log:TRAIN Epoch: 230 Loss: 0.5758712371190389\n",
      "INFO:dkt log:EVAL  Epoch: 230 Loss: 0.5770984093348185\n",
      "INFO:dkt log:EVAL  Epoch: 230 AUC: 0.7426607801192482\n",
      "INFO:dkt log:52m 27s ( - 61m 35s) (230 46.0)\n",
      "INFO:dkt log:TRAIN Epoch: 240 Loss: 0.5738822301228841\n",
      "INFO:dkt log:EVAL  Epoch: 240 Loss: 0.5759202043215433\n",
      "INFO:dkt log:EVAL  Epoch: 240 AUC: 0.7437986436666516\n",
      "INFO:dkt log:56m 23s ( - 61m 4s) (240 48.0)\n",
      "INFO:dkt log:TRAIN Epoch: 250 Loss: 0.5729915181795756\n",
      "INFO:dkt log:EVAL  Epoch: 250 Loss: 0.5747371514638265\n",
      "INFO:dkt log:EVAL  Epoch: 250 AUC: 0.7472577488507577\n",
      "INFO:dkt log:60m 26s ( - 60m 26s) (250 50.0)\n",
      "INFO:dkt log:TRAIN Epoch: 260 Loss: 0.5748124241828918\n",
      "INFO:dkt log:EVAL  Epoch: 260 Loss: 0.5744691292444865\n",
      "INFO:dkt log:EVAL  Epoch: 260 AUC: 0.7467570888899003\n",
      "INFO:dkt log:64m 23s ( - 59m 26s) (260 52.0)\n",
      "INFO:dkt log:TRAIN Epoch: 270 Loss: 0.5665110945701599\n",
      "INFO:dkt log:EVAL  Epoch: 270 Loss: 0.572718878587087\n",
      "INFO:dkt log:EVAL  Epoch: 270 AUC: 0.7492148741522917\n",
      "INFO:dkt log:68m 17s ( - 58m 10s) (270 54.0)\n",
      "INFO:dkt log:TRAIN Epoch: 280 Loss: 0.572512952486674\n",
      "INFO:dkt log:EVAL  Epoch: 280 Loss: 0.5718888839085897\n",
      "INFO:dkt log:EVAL  Epoch: 280 AUC: 0.750671339492968\n",
      "INFO:dkt log:72m 11s ( - 56m 43s) (280 56.00000000000001)\n",
      "INFO:dkt log:TRAIN Epoch: 290 Loss: 0.5652348399162292\n",
      "INFO:dkt log:EVAL  Epoch: 290 Loss: 0.5713448524475098\n",
      "INFO:dkt log:EVAL  Epoch: 290 AUC: 0.7508989122024488\n",
      "INFO:dkt log:76m 5s ( - 55m 6s) (290 57.99999999999999)\n",
      "Level 25:dkt log:TRAIN Epoch: 300 Loss: 0.5621381839116414\n",
      "Level 25:dkt log:EVAL  Epoch: 300 Loss: 0.5698685050010681\n",
      "Level 25:dkt log:EVAL  Epoch: 300 AUC: 0.7531291247553593\n",
      "INFO:dkt log:80m 4s ( - 53m 22s) (300 60.0)\n",
      "INFO:dkt log:TRAIN Epoch: 310 Loss: 0.5655036807060242\n",
      "INFO:dkt log:EVAL  Epoch: 310 Loss: 0.569431742032369\n",
      "INFO:dkt log:EVAL  Epoch: 310 AUC: 0.7537208138000091\n",
      "INFO:dkt log:84m 1s ( - 51m 29s) (310 62.0)\n",
      "INFO:dkt log:TRAIN Epoch: 320 Loss: 0.5607846836249034\n",
      "INFO:dkt log:EVAL  Epoch: 320 Loss: 0.567756175994873\n",
      "INFO:dkt log:EVAL  Epoch: 320 AUC: 0.7548131628055164\n",
      "INFO:dkt log:87m 59s ( - 49m 29s) (320 64.0)\n",
      "INFO:dkt log:TRAIN Epoch: 330 Loss: 0.5663004239400228\n",
      "INFO:dkt log:EVAL  Epoch: 330 Loss: 0.5670913259188334\n",
      "INFO:dkt log:EVAL  Epoch: 330 AUC: 0.754722133721724\n",
      "INFO:dkt log:91m 57s ( - 47m 22s) (330 66.0)\n",
      "INFO:dkt log:TRAIN Epoch: 340 Loss: 0.5587866385777791\n",
      "INFO:dkt log:EVAL  Epoch: 340 Loss: 0.56583704551061\n",
      "INFO:dkt log:EVAL  Epoch: 340 AUC: 0.7568613171908426\n",
      "INFO:dkt log:95m 55s ( - 45m 8s) (340 68.0)\n",
      "INFO:dkt log:TRAIN Epoch: 350 Loss: 0.5584674795468648\n",
      "INFO:dkt log:EVAL  Epoch: 350 Loss: 0.5647806326548258\n",
      "INFO:dkt log:EVAL  Epoch: 350 AUC: 0.7569068317327385\n",
      "INFO:dkt log:99m 54s ( - 42m 48s) (350 70.0)\n",
      "INFO:dkt log:TRAIN Epoch: 360 Loss: 0.5641766548156738\n",
      "INFO:dkt log:EVAL  Epoch: 360 Loss: 0.563675065835317\n",
      "INFO:dkt log:EVAL  Epoch: 360 AUC: 0.7580902098220382\n",
      "INFO:dkt log:103m 52s ( - 40m 23s) (360 72.0)\n",
      "INFO:dkt log:TRAIN Epoch: 370 Loss: 0.5554123063882191\n",
      "INFO:dkt log:EVAL  Epoch: 370 Loss: 0.5631126761436462\n",
      "INFO:dkt log:EVAL  Epoch: 370 AUC: 0.7602749078330526\n",
      "INFO:dkt log:107m 51s ( - 37m 53s) (370 74.0)\n",
      "INFO:dkt log:TRAIN Epoch: 380 Loss: 0.5584983706474305\n",
      "INFO:dkt log:EVAL  Epoch: 380 Loss: 0.5626150369644165\n",
      "INFO:dkt log:EVAL  Epoch: 380 AUC: 0.7607755677939102\n",
      "INFO:dkt log:111m 49s ( - 35m 18s) (380 76.0)\n",
      "INFO:dkt log:TRAIN Epoch: 390 Loss: 0.5519358654816945\n",
      "INFO:dkt log:EVAL  Epoch: 390 Loss: 0.5636618932088217\n",
      "INFO:dkt log:EVAL  Epoch: 390 AUC: 0.7602293932911566\n",
      "INFO:dkt log:115m 48s ( - 32m 39s) (390 78.0)\n",
      "Level 25:dkt log:TRAIN Epoch: 400 Loss: 0.5567447940508524\n",
      "Level 25:dkt log:EVAL  Epoch: 400 Loss: 0.5638580918312073\n",
      "Level 25:dkt log:EVAL  Epoch: 400 AUC: 0.7588184424923763\n",
      "INFO:dkt log:119m 47s ( - 29m 56s) (400 80.0)\n",
      "INFO:dkt log:TRAIN Epoch: 410 Loss: 0.5554529587427776\n",
      "INFO:dkt log:EVAL  Epoch: 410 Loss: 0.5624592900276184\n",
      "INFO:dkt log:EVAL  Epoch: 410 AUC: 0.7615038004642483\n",
      "INFO:dkt log:123m 46s ( - 27m 10s) (410 82.0)\n",
      "INFO:dkt log:TRAIN Epoch: 420 Loss: 0.5518196165561676\n",
      "INFO:dkt log:EVAL  Epoch: 420 Loss: 0.5631619095802307\n",
      "INFO:dkt log:EVAL  Epoch: 420 AUC: 0.760365936916845\n",
      "INFO:dkt log:127m 45s ( - 24m 20s) (420 84.0)\n",
      "INFO:dkt log:TRAIN Epoch: 430 Loss: 0.5492566188176473\n",
      "INFO:dkt log:EVAL  Epoch: 430 Loss: 0.5626888871192932\n",
      "INFO:dkt log:EVAL  Epoch: 430 AUC: 0.7622775476764828\n",
      "INFO:dkt log:131m 44s ( - 21m 26s) (430 86.0)\n",
      "INFO:dkt log:TRAIN Epoch: 440 Loss: 0.5523383994897206\n",
      "INFO:dkt log:EVAL  Epoch: 440 Loss: 0.5637645920117696\n",
      "INFO:dkt log:EVAL  Epoch: 440 AUC: 0.7592280733694414\n",
      "INFO:dkt log:135m 42s ( - 18m 30s) (440 88.0)\n",
      "INFO:dkt log:TRAIN Epoch: 450 Loss: 0.5438606798648834\n",
      "INFO:dkt log:EVAL  Epoch: 450 Loss: 0.5643996993700663\n",
      "INFO:dkt log:EVAL  Epoch: 450 AUC: 0.7591825588275454\n",
      "INFO:dkt log:139m 55s ( - 15m 32s) (450 90.0)\n",
      "INFO:dkt log:TRAIN Epoch: 460 Loss: 0.551059901714325\n",
      "INFO:dkt log:EVAL  Epoch: 460 Loss: 0.5643999576568604\n",
      "INFO:dkt log:EVAL  Epoch: 460 AUC: 0.758044695280142\n",
      "INFO:dkt log:144m 11s ( - 12m 32s) (460 92.0)\n",
      "INFO:dkt log:TRAIN Epoch: 470 Loss: 0.5449002067248027\n",
      "INFO:dkt log:EVAL  Epoch: 470 Loss: 0.5648161172866821\n",
      "INFO:dkt log:EVAL  Epoch: 470 AUC: 0.7590915297437533\n",
      "INFO:dkt log:148m 10s ( - 9m 27s) (470 94.0)\n",
      "INFO:dkt log:TRAIN Epoch: 480 Loss: 0.5409541209538777\n",
      "INFO:dkt log:EVAL  Epoch: 480 Loss: 0.56548939148585\n",
      "INFO:dkt log:EVAL  Epoch: 480 AUC: 0.75936461699513\n",
      "INFO:dkt log:152m 10s ( - 6m 20s) (480 96.0)\n",
      "INFO:dkt log:TRAIN Epoch: 490 Loss: 0.54330499569575\n",
      "INFO:dkt log:EVAL  Epoch: 490 Loss: 0.5655297239621481\n",
      "INFO:dkt log:EVAL  Epoch: 490 AUC: 0.7602293932911565\n",
      "INFO:dkt log:156m 9s ( - 3m 11s) (490 98.0)\n",
      "Level 25:dkt log:TRAIN Epoch: 500 Loss: 0.5426633199055989\n",
      "Level 25:dkt log:EVAL  Epoch: 500 Loss: 0.5662286480267843\n",
      "Level 25:dkt log:EVAL  Epoch: 500 AUC: 0.760001820581676\n",
      "INFO:dkt log:160m 8s ( - 0m 0s) (500 100.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40 40\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# Model\n",
    "# =========================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, emb_dim, hid_dim, n_layers, dropout):\n",
    "        # def __init__(self, dev, model_name, n_input, n_hidden, n_output, n_layers, batch_size, dropout=0.6, bidirectional=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        # def __init__(self, dev, model_name, n_input, n_hidden, n_output, n_layers, batch_size, dropout=0.6, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)  # 250->6\n",
    "        \n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)  # 6, 100, 1\n",
    "        \n",
    "        self.out = nn.Linear(hid_dim, output_dim)  # 100, 250\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        #print(input.shape) #torch.Size([21])\n",
    "        input = input.unsqueeze(0)\n",
    "        #print(input.shape) #torch.Size([1, 21]) \n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = self.dropout(input)\n",
    "        #print(embedded.shape)  # torch.Size([1, 15, 6])\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "         \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #sent len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        prediction = self.out(output.squeeze(0))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell\n",
    "\n",
    "    \n",
    "OUTP = None\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, dev):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = dev\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "    \n",
    "    def forward(self, src, trg, actual_q=None, teacher_forcing_ratio=0.5):\n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "#         print(f'target shape: {trg.shape}')\n",
    "#         print(f'batch_size: {batch_size}, max_len: {max_len}')\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "#         print(f'vocab size: {trg_vocab_size}')\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)  # TODO: fix hard coding\n",
    "        outputs_prob = torch.zeros(max_len, batch_size, 124).to(self.device)  # TODO: fix hard coding\n",
    "#         print('s2s outputs shape:', outputs.shape)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # #first input to the decoder is the <sos> tokens\n",
    "        # input = trg[0,:]\n",
    "        # \n",
    "        # # print(actual_q.shape) # 100, 20, 124\n",
    "        # for t in range(1, max_len):\n",
    "        #     \n",
    "        #     output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "        #     # print(output.shape) # 100, 250\n",
    "        #     # つまり100ごとにバッチ処理をしていて、20のSeqを頭から順に処理している段階\n",
    "        #     #global OUTP\n",
    "        #     #OUTP = output\n",
    "        #     outputs[t] = output\n",
    "        #     o_wro = torch.sigmoid(output[:, 2:2+124])\n",
    "        #     o_cor = torch.sigmoid(output[:, 2+124:])\n",
    "        #     outputs_prob[t] = o_cor / (o_cor + o_wro)\n",
    "        #     teacher_force = random.random() < teacher_forcing_ratio\n",
    "        #     top1 = output.max(1)[1]\n",
    "        #     flag = torch.zeros(100, 2)  # PRESERVED_TAGS = 2\n",
    "        #     flag = torch.cat((flag, actual_q[:,t], actual_q[:,t]), dim=1)\n",
    "        #     top1 = torch.max(torch.sigmoid(output) * flag, dim=1)[1]\n",
    "        #     input = (trg[t] if teacher_force else top1)\n",
    "        # print(actual_q.shape) # 100, 20, 124\n",
    "            \n",
    "        input = trg[-2,:]\n",
    "         \n",
    "        output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "        # print(output.shape) # 100, 250\n",
    "        # つまり100ごとにバッチ処理をしていて、20のSeqを頭から順に処理している段階\n",
    "        #global OUTP\n",
    "        #OUTP = output\n",
    "        outputs = output.unsqueeze(0)\n",
    "        o_wro = torch.sigmoid(output[:, 2:2+124])\n",
    "        o_cor = torch.sigmoid(output[:, 2+124:])\n",
    "        outputs_prob = (o_cor / (o_cor + o_wro)).unsqueeze(0)\n",
    "        \n",
    "        return outputs, outputs_prob\n",
    "    \n",
    "    \n",
    "\n",
    "# =========================\n",
    "# Prepare and Train\n",
    "# =========================\n",
    "enc = Encoder(NUM_EMBEDDIGNS, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(dev)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(dev)\n",
    "\n",
    "model = Seq2Seq(enc, dec, dev).to(dev)\n",
    "\n",
    "# Load model\n",
    "# ----------\n",
    "load_model = None\n",
    "epoch_start = 1\n",
    "\n",
    "load_model = '/home/qqhann/qqhann-paper/ECML2019/dkt_neo/models/s2s_2019_0404_2021.100'\n",
    "if load_model:\n",
    "    epoch_start = int(load_model.split('.')[-1]) + 1\n",
    "    model.load_state_dict(torch.load(load_model))\n",
    "    model = model.to(dev)\n",
    "# ----------\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "opt = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "def train():\n",
    "    pass\n",
    "\n",
    "def evaluate():\n",
    "    pass\n",
    "\n",
    "PRED = None\n",
    "def main():\n",
    "    debug = False \n",
    "    logging.basicConfig()\n",
    "    logger = logging.getLogger('dkt log')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    train_loss_list = []\n",
    "    train_auc_list = []\n",
    "    eval_loss_list = []\n",
    "    eval_auc_list = []\n",
    "    eval_recall_list = []\n",
    "    eval_f1_list = []\n",
    "    x = []\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_start, epoch_size + 1):\n",
    "        print_train = epoch % 10 == 0\n",
    "        print_eval = epoch % 10 == 0\n",
    "        print_auc = epoch % 10 == 0\n",
    "\n",
    "        \n",
    "        # =====\n",
    "        # TRAIN\n",
    "        # =====\n",
    "        model.train()\n",
    "\n",
    "        val_pred = []\n",
    "        val_actual = []\n",
    "        current_epoch_train_loss = []\n",
    "        for xs, ys, yq, ya, yp in train_dl:\n",
    "            input = xs\n",
    "            target = ys\n",
    "            input = input.permute(1, 0)\n",
    "            target = target.permute(1, 0)\n",
    "\n",
    "            out, out_prob = model(input, target, yq)\n",
    "            out = out.permute(1, 0, 2)\n",
    "            out_prob = out_prob.permute(1, 0, 2)\n",
    "\n",
    "            pred = torch.sigmoid(out)  # [0, 1]区間にする\n",
    "            # _, pred = torch.max(pred, 2)\n",
    "            target = torch.tensor([list(torch.eye(NUM_EMBEDDIGNS)[i]) for i in target.contiguous().view(-1)])\\\n",
    "                .contiguous().view(batch_size, -1, NUM_EMBEDDIGNS).to(dev)\n",
    "            \n",
    "            # --- 指標評価用データ\n",
    "#             print(out_prob.shape, yq[:,-1,:].unsqueeze(1).shape)\n",
    "            prob = torch.max(out_prob * yq[:,-1,:].unsqueeze(1), 2)[0]\n",
    "            val_pred.append(prob)\n",
    "            val_actual.append(ya[:,-1])\n",
    "            # ---\n",
    "            \n",
    "#             print(prob.shape, ya.shape)\n",
    "            loss = loss_func(prob[:,-1], ya[:,-1]) \n",
    "            \n",
    "            current_epoch_train_loss.append(loss.item())\n",
    "            \n",
    "\n",
    "            # バックプロバゲーション\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # stop at first batch if debug\n",
    "            if debug:\n",
    "                break\n",
    "                \n",
    "        if print_train:\n",
    "            loss = np.array(current_epoch_train_loss)\n",
    "            logger.log(logging.INFO + (5 if epoch % 100 == 0 else 0),\n",
    "                       'TRAIN Epoch: {} Loss: {}'.format(epoch, loss.mean()))\n",
    "            train_loss_list.append(loss.mean())\n",
    "            \n",
    "            # # AUC, Recall, F1\n",
    "            # # TRAINの場合、勾配があるから処理が必要\n",
    "            # y = torch.cat(val_targ).cpu().detach().numpy()\n",
    "            # pred = torch.cat(val_prob).cpu().detach().numpy()\n",
    "            # # AUC\n",
    "            # fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "            # logger.log(logging.INFO + (5 if epoch % 100 == 0 else 0), \n",
    "            #            'TRAIN Epoch: {} AUC: {}'.format(epoch, metrics.auc(fpr, tpr)))\n",
    "            # train_auc_list.append(metrics.auc(fpr, tpr))\n",
    "\n",
    "\n",
    "        # =====\n",
    "        # EVAL\n",
    "        # =====\n",
    "        if print_eval:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "\n",
    "                val_pred = []\n",
    "                val_actual = []\n",
    "                current_eval_loss = []\n",
    "                for xs, ys, yq, ya, yp in eval_dl:\n",
    "                    input = xs\n",
    "                    target = ys\n",
    "                    input = input.permute(1, 0)\n",
    "                    target = target.permute(1, 0)\n",
    "\n",
    "                    out, out_prob = model(input, target, yq)\n",
    "                    out = out.permute(1, 0, 2)\n",
    "                    out_prob = out_prob.permute(1, 0, 2)\n",
    "\n",
    "                    pred = torch.sigmoid(out)  # [0, 1]区間にする\n",
    "                    # _, pred = torch.max(pred, 2)\n",
    "                    target = torch.tensor([list(torch.eye(NUM_EMBEDDIGNS)[i]) for i in target.contiguous().view(-1)])\\\n",
    "                        .contiguous().view(batch_size, -1, NUM_EMBEDDIGNS).to(dev)\n",
    "\n",
    "                    # --- 指標評価用データ\n",
    "                    prob = torch.max(out_prob * yq[:,-1,:].unsqueeze(1), 2)[0]\n",
    "                    val_pred.append(prob)\n",
    "                    val_actual.append(ya[:,-1])\n",
    "                    # ---\n",
    "\n",
    "        #             print(prob.shape, ya.shape)\n",
    "                    loss = loss_func(prob[:,-1], ya[:,-1]) \n",
    "    \n",
    "                    current_eval_loss.append(loss.item())\n",
    "\n",
    "                    # stop at first batch if debug\n",
    "                    if debug:\n",
    "                        break\n",
    "                        \n",
    "                loss = np.array(current_eval_loss)\n",
    "                logger.log(logging.INFO + (5 if epoch % 100 == 0 else 0), \n",
    "                           'EVAL  Epoch: {} Loss: {}'.format(epoch,  loss.mean()))\n",
    "                eval_loss_list.append(loss.mean())\n",
    "\n",
    "                # AUC, Recall, F1\n",
    "                if print_auc:\n",
    "                    y = torch.cat(val_actual).view(-1).cpu()  # TODO: viewしない？　最後の1個で？\n",
    "                    pred = torch.cat(val_pred).view(-1).cpu()\n",
    "                    # AUC\n",
    "                    fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=1)\n",
    "                    logger.log(logging.INFO + (5 if epoch % 100 == 0 else 0), \n",
    "                               'EVAL  Epoch: {} AUC: {}'.format(epoch, metrics.auc(fpr, tpr)))\n",
    "                    eval_auc_list.append(metrics.auc(fpr, tpr))\n",
    "                #     # Recall\n",
    "                #     logger.debug('EVAL  Epoch: {} Recall: {}'.format(epoch, metrics.recall_score(y, pred.round())))\n",
    "                #     # F1 score\n",
    "                #     logger.debug('EVAL  Epoch: {} F1 score: {}'.format(epoch, metrics.f1_score(y, pred.round())))\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            x.append(epoch)\n",
    "            logger.info(f'{timeSince(start_time, epoch / epoch_size)} ({epoch} {epoch / epoch_size * 100})')\n",
    "\n",
    "        \n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(x, train_loss_list, label='train loss')\n",
    "    # ax.plot(x, train_auc_list, label='train auc')\n",
    "    ax.plot(x, eval_loss_list, label='eval loss')\n",
    "    ax.plot(x, eval_auc_list, label='eval auc')\n",
    "    ax.legend()\n",
    "    print(len(train_loss_list), len(eval_loss_list), len(eval_auc_list))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('Using Device:', dev)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(250, 6)\n",
       "    (rnn): LSTM(6, 200, dropout=0.6)\n",
       "    (dropout): Dropout(p=0.6)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(250, 6)\n",
       "    (rnn): LSTM(6, 200, dropout=0.6)\n",
       "    (out): Linear(in_features=200, out_features=250, bias=True)\n",
       "    (dropout): Dropout(p=0.6)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now().strftime('%Y_%m%d_%H%M')\n",
    "\n",
    "torch.save(model.state_dict(), '/home/qqhann/qqhann-paper/ECML2019/dkt_neo/models/s2s_' + now + '.' + str(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
